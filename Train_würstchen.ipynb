{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dDnD7npf7ix_",
      "metadata": {
        "id": "dDnD7npf7ix_"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/korakoe/Wuerstchen/blob/main/Train_w%C3%BCrstchen.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ShoukanLabs/.github/main/profile/ShoukanLab-circle.png\" width=\"256\"></p>\n",
        "\n",
        "<p align=\"center\", style=\"font-size: 2rem; font-weight: bold; color: #ff593e;\">üß™ ShoukanLabs „ÉºÂè¨ÂñöLabs„Éº</p>\n",
        "\n",
        "<p align=\"center\">Shoukan Âè¨Âñö„Äê„Åó„Çá„ÅÜ„Åã„Çì„Äë „Éº can be translated into either Summon or Summoning.</p>\n",
        "\n",
        "<p align=\"center\" style=\"font-size: 1rem; font-weight: bold; color: #ff593e;\">Join our Discord at: https://discord.gg/5bq9HqVhsJ</p>\n",
        "\n",
        "---\n",
        "## üîé What is this notebook?\n",
        "\n",
        "This is a small training notebook for wurstchen, as the official training script requires ddp, and doesnt have argparse, as such ive taken the time to add these to the script, you can find my github [here](https://github.com/korakoe/Wuerstchen)\n",
        "\n",
        "---\n",
        "\n",
        "#### üìö In the mean time... check out our other projects\n",
        "\n",
        "- **Future Projects**\n",
        "  - Unnamed TTS project\n",
        "  - OpenNiji-V3 *(placeholder name)*\n",
        "\n",
        "- **Current Projects**\n",
        "  - [OpenNiji-Dataset](https://huggingface.co/datasets/ShoukanLabs/OpenNiji-Dataset) - The dataset that will be used for OpenNiji-V3\n",
        "    - [Dataset preview](https://huggingface.co/spaces/ShoukanLabs/OpenNiji-Dataset-Viewer) - See what images the dataset contains\n",
        "\n",
        "- **Previous Projects**\n",
        "  - [OpenNiji](https://huggingface.co/ShoukanLabs/OpenNiji) - A finetune aimed at replicating Nijijourney on Stable Diffusion.\n",
        "  - [OpenNiji-V2](https://huggingface.co/ShoukanLabs/OpenNiji-V2) - A second finetune made to replicate the Nijijourney style more accurately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "c6200fa6-a1b7-42f1-a5dc-8eee985e0123",
      "metadata": {
        "cellView": "form",
        "id": "c6200fa6-a1b7-42f1-a5dc-8eee985e0123",
        "outputId": "dcfb4513-4891-4882-a0c3-f51f23c40ff8",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Wuerstchen'...\n",
            "remote: Enumerating objects: 200, done.\u001b[K\n",
            "remote: Counting objects: 100% (108/108), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 200 (delta 63), reused 64 (delta 30), pack-reused 92\u001b[K\n",
            "Receiving objects: 100% (200/200), 44.86 MiB | 26.22 MiB/s, done.\n",
            "Resolving deltas: 100% (110/110), done.\n",
            "Collecting pytorch-tools@ git+https://github.com/pabloppp/pytorch-tools@master (from -r Wuerstchen/requirements.txt (line 4))\n",
            "  Cloning https://github.com/pabloppp/pytorch-tools (to revision master) to /tmp/pip-install-3ju0oc4n/pytorch-tools_9215b6e24d9347e0b3c24dbcc606f564\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pabloppp/pytorch-tools /tmp/pip-install-3ju0oc4n/pytorch-tools_9215b6e24d9347e0b3c24dbcc606f564\n",
            "  Resolved https://github.com/pabloppp/pytorch-tools to commit c99c4c1779b66429044cc686fc94c659bc5f5dee\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package pytorch-tools produced metadata for project name torchtools. Fix your #egg=pytorch-tools fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mgit+https://github.com/pabloppp/pytorch-tools@master\u001b[0m: \u001b[33mRequested torchtools from git+https://github.com/pabloppp/pytorch-tools@master (from -r Wuerstchen/requirements.txt (line 4)) has inconsistent name: expected 'pytorch-tools', but metadata has 'torchtools'\u001b[0m\n",
            "Requirement already satisfied: webdataset in /usr/local/lib/python3.10/dist-packages (from -r Wuerstchen/requirements.txt (line 1)) (0.2.57)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r Wuerstchen/requirements.txt (line 2)) (4.33.3)\n",
            "Requirement already satisfied: warmup_scheduler in /usr/local/lib/python3.10/dist-packages (from -r Wuerstchen/requirements.txt (line 3)) (0.3)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch-tools (unavailable) (from versions: 0.1.4, 0.1.5, 0.1.7, 0.1.8, 0.1.9, 0.5.1, 0.5.3, 0.5.5, 0.5.6)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch-tools (unavailable)\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting git+https://github.com/pabloppp/pytorch-tools@master\n",
            "  Cloning https://github.com/pabloppp/pytorch-tools (to revision master) to /tmp/pip-req-build-dvowsc5j\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pabloppp/pytorch-tools /tmp/pip-req-build-dvowsc5j\n",
            "  Resolved https://github.com/pabloppp/pytorch-tools to commit c99c4c1779b66429044cc686fc94c659bc5f5dee\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Requirement already satisfied: install in /usr/local/lib/python3.10/dist-packages (1.3.5)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from torchtools==0.3.6) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from torchtools==0.3.6) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.10/dist-packages (from torchtools==0.3.6) (1.23.5)\n",
            "Requirement already satisfied: ninja>=1.0 in /usr/local/lib/python3.10/dist-packages (from torchtools==0.3.6) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->torchtools==0.3.6) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->torchtools==0.3.6) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->torchtools==0.3.6) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->torchtools==0.3.6) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->torchtools==0.3.6) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->torchtools==0.3.6) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6->torchtools==0.3.6) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6->torchtools==0.3.6) (16.0.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->torchtools==0.3.6) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->torchtools==0.3.6) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->torchtools==0.3.6) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->torchtools==0.3.6) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->torchtools==0.3.6) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->torchtools==0.3.6) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->torchtools==0.3.6) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->torchtools==0.3.6) (1.3.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.11)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.33.3)\n",
            "Requirement already satisfied: webdataset in /usr/local/lib/python3.10/dist-packages (0.2.57)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: warmup_scheduler in /usr/local/lib/python3.10/dist-packages (0.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.37)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.31.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: braceexpand in /usr/local/lib/python3.10/dist-packages (from webdataset) (0.1.7)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Logging into W&B...\n",
            "Mounting Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#@title Create Ennvrionment\n",
        "\n",
        "!rm -r Wuerstchen\n",
        "!git clone https://github.com/korakoe/Wuerstchen.git\n",
        "!pip install -r Wuerstchen/requirements.txt\n",
        "!pip install pip install git+https://github.com/pabloppp/pytorch-tools@master\n",
        "!pip install wandb transformers webdataset datasets warmup_scheduler\n",
        "\n",
        "import wandb\n",
        "print(\"Logging into W&B...\")\n",
        "wandb.login()\n",
        "\n",
        "from google.colab import drive\n",
        "print(\"Mounting Drive...\")\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Required Models For Training\n",
        "!wget https://huggingface.co/dome272/wuerstchen/resolve/main/vqgan_f4_v1_500k.pt -O \"vq.pt\"\n",
        "!wget https://huggingface.co/dome272/wuerstchen/resolve/main/model_stage_b.pt -O \"stage_b.bin\"\n",
        "!wget https://huggingface.co/dome272/wuerstchen/resolve/main/model_stage_c.pt -O \"stage_c.bin\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "PW7YArUCtjT_",
        "outputId": "86cce4b3-d7d3-47c8-9582-9ef0314782d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PW7YArUCtjT_",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-30 11:00:38--  https://huggingface.co/dome272/wuerstchen/resolve/main/vqgan_f4_v1_500k.pt\n",
            "Resolving huggingface.co (huggingface.co)... 52.84.162.101, 52.84.162.114, 52.84.162.49, ...\n",
            "Connecting to huggingface.co (huggingface.co)|52.84.162.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/b9/6e/b96ebc9f17a640b8e89f419a072a1ef4e6dadbcdf69a668bf2518fc87cd52c63/b4a81dd8733268a68d08844e33b917d4dbe27d5c5382dcbde522df3af16f343e?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27vqgan_f4_v1_500k.pt%3B+filename%3D%22vqgan_f4_v1_500k.pt%22%3B&Expires=1696330838&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NjMzMDgzOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iOS82ZS9iOTZlYmM5ZjE3YTY0MGI4ZTg5ZjQxOWEwNzJhMWVmNGU2ZGFkYmNkZjY5YTY2OGJmMjUxOGZjODdjZDUyYzYzL2I0YTgxZGQ4NzMzMjY4YTY4ZDA4ODQ0ZTMzYjkxN2Q0ZGJlMjdkNWM1MzgyZGNiZGU1MjJkZjNhZjE2ZjM0M2U%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=sRRc2zigf0Ea0e2Pm2uGNVP6kY%7E0q03l8XMhJHQ3fjFJqPHA5WAwn03FS5M2444tVMaPo-pzLRiaRUp70Opfqipx4kG0P2%7EoircYU8rCvxyF51-LiTBXzBzCVKCdtqDGQEZblvL3YFkH7D-W90hLW9pXTgOEdrwMUcF0m2sOlOPep1TqNaPjoiq%7ER4oOIdfwlSlLPJdOs9G10hdeslwh-C77bQ1WoVl4OKprTEOjwVBqZsTZsX2oCzEO%7ElDXO0C0gW91puWd4ybDv8hXo39e47ixaXQYFi30eQ38bTyNgoruwM6s7795-LsExbOK7Xx%7EoApMS2H6lUAFr9ggi1HQIA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-09-30 11:00:38--  https://cdn-lfs.huggingface.co/repos/b9/6e/b96ebc9f17a640b8e89f419a072a1ef4e6dadbcdf69a668bf2518fc87cd52c63/b4a81dd8733268a68d08844e33b917d4dbe27d5c5382dcbde522df3af16f343e?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27vqgan_f4_v1_500k.pt%3B+filename%3D%22vqgan_f4_v1_500k.pt%22%3B&Expires=1696330838&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NjMzMDgzOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iOS82ZS9iOTZlYmM5ZjE3YTY0MGI4ZTg5ZjQxOWEwNzJhMWVmNGU2ZGFkYmNkZjY5YTY2OGJmMjUxOGZjODdjZDUyYzYzL2I0YTgxZGQ4NzMzMjY4YTY4ZDA4ODQ0ZTMzYjkxN2Q0ZGJlMjdkNWM1MzgyZGNiZGU1MjJkZjNhZjE2ZjM0M2U%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=sRRc2zigf0Ea0e2Pm2uGNVP6kY%7E0q03l8XMhJHQ3fjFJqPHA5WAwn03FS5M2444tVMaPo-pzLRiaRUp70Opfqipx4kG0P2%7EoircYU8rCvxyF51-LiTBXzBzCVKCdtqDGQEZblvL3YFkH7D-W90hLW9pXTgOEdrwMUcF0m2sOlOPep1TqNaPjoiq%7ER4oOIdfwlSlLPJdOs9G10hdeslwh-C77bQ1WoVl4OKprTEOjwVBqZsTZsX2oCzEO%7ElDXO0C0gW91puWd4ybDv8hXo39e47ixaXQYFi30eQ38bTyNgoruwM6s7795-LsExbOK7Xx%7EoApMS2H6lUAFr9ggi1HQIA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.65.229.35, 18.65.229.16, 18.65.229.73, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.65.229.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 296367311 (283M) [binary/octet-stream]\n",
            "Saving to: ‚Äòvq.pt‚Äô\n",
            "\n",
            "vq.pt               100%[===================>] 282.64M   247MB/s    in 1.1s    \n",
            "\n",
            "2023-09-30 11:00:39 (247 MB/s) - ‚Äòvq.pt‚Äô saved [296367311/296367311]\n",
            "\n",
            "--2023-09-30 11:00:39--  https://huggingface.co/dome272/wuerstchen/resolve/main/model_stage_b.pt\n",
            "Resolving huggingface.co (huggingface.co)... 52.84.162.101, 52.84.162.114, 52.84.162.49, ...\n",
            "Connecting to huggingface.co (huggingface.co)|52.84.162.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/b9/6e/b96ebc9f17a640b8e89f419a072a1ef4e6dadbcdf69a668bf2518fc87cd52c63/6cfd09dbe8765a7f973dc96f7b1f2e8328e35bab414f556d67d994676c19c6d5?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model_stage_b.pt%3B+filename%3D%22model_stage_b.pt%22%3B&Expires=1696330839&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NjMzMDgzOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iOS82ZS9iOTZlYmM5ZjE3YTY0MGI4ZTg5ZjQxOWEwNzJhMWVmNGU2ZGFkYmNkZjY5YTY2OGJmMjUxOGZjODdjZDUyYzYzLzZjZmQwOWRiZTg3NjVhN2Y5NzNkYzk2ZjdiMWYyZTgzMjhlMzViYWI0MTRmNTU2ZDY3ZDk5NDY3NmMxOWM2ZDU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Cds6XgCON3DmLg0QZ3rrA2c9oIL023C3kzD6n2cf61zz%7Ez3IcF3AT7S7QmdNhj6wABaaX3NbaC4ei4OKQqFz14PY-c9DJpeOEhiGH3EuEeEkew3uzN7ohx2%7E4FGjGfC3jjNY7R4zfoyyLNLj1I5nmDc7GSxNCldbBvTe02hb0QHTYdJ67yzXFitIx3zq71Eye3mE9hrPxvqarkp6VRHB0PmuU6SsOFJNcJNRLk-yCZRPLnNaLkQaTjGMozM77C2u3htZSulN4uxArm8sEATKKdsTL%7E3JxM47PaXxtVG3NIqeakS66f45iHs6HhM-Fs9wqE6rUBOpCHgMxEFjuu6Kew__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-09-30 11:00:39--  https://cdn-lfs.huggingface.co/repos/b9/6e/b96ebc9f17a640b8e89f419a072a1ef4e6dadbcdf69a668bf2518fc87cd52c63/6cfd09dbe8765a7f973dc96f7b1f2e8328e35bab414f556d67d994676c19c6d5?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model_stage_b.pt%3B+filename%3D%22model_stage_b.pt%22%3B&Expires=1696330839&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NjMzMDgzOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iOS82ZS9iOTZlYmM5ZjE3YTY0MGI4ZTg5ZjQxOWEwNzJhMWVmNGU2ZGFkYmNkZjY5YTY2OGJmMjUxOGZjODdjZDUyYzYzLzZjZmQwOWRiZTg3NjVhN2Y5NzNkYzk2ZjdiMWYyZTgzMjhlMzViYWI0MTRmNTU2ZDY3ZDk5NDY3NmMxOWM2ZDU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Cds6XgCON3DmLg0QZ3rrA2c9oIL023C3kzD6n2cf61zz%7Ez3IcF3AT7S7QmdNhj6wABaaX3NbaC4ei4OKQqFz14PY-c9DJpeOEhiGH3EuEeEkew3uzN7ohx2%7E4FGjGfC3jjNY7R4zfoyyLNLj1I5nmDc7GSxNCldbBvTe02hb0QHTYdJ67yzXFitIx3zq71Eye3mE9hrPxvqarkp6VRHB0PmuU6SsOFJNcJNRLk-yCZRPLnNaLkQaTjGMozM77C2u3htZSulN4uxArm8sEATKKdsTL%7E3JxM47PaXxtVG3NIqeakS66f45iHs6HhM-Fs9wqE6rUBOpCHgMxEFjuu6Kew__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.65.229.35, 18.65.229.16, 18.65.229.73, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.65.229.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3395142175 (3.2G) [binary/octet-stream]\n",
            "Saving to: ‚Äòstage_b.bin‚Äô\n",
            "\n",
            "stage_b.bin         100%[===================>]   3.16G   144MB/s    in 17s     \n",
            "\n",
            "2023-09-30 11:00:57 (189 MB/s) - ‚Äòstage_b.bin‚Äô saved [3395142175/3395142175]\n",
            "\n",
            "--2023-09-30 11:00:57--  https://huggingface.co/dome272/wuerstchen/resolve/main/model_stage_c.pt\n",
            "Resolving huggingface.co (huggingface.co)... 52.84.162.101, 52.84.162.114, 52.84.162.49, ...\n",
            "Connecting to huggingface.co (huggingface.co)|52.84.162.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/b9/6e/b96ebc9f17a640b8e89f419a072a1ef4e6dadbcdf69a668bf2518fc87cd52c63/41adcb61260c2728b5b09136b30cf60f4f98ac8eb05643450028625c8a7615c0?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model_stage_c.pt%3B+filename%3D%22model_stage_c.pt%22%3B&Expires=1696330857&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NjMzMDg1N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iOS82ZS9iOTZlYmM5ZjE3YTY0MGI4ZTg5ZjQxOWEwNzJhMWVmNGU2ZGFkYmNkZjY5YTY2OGJmMjUxOGZjODdjZDUyYzYzLzQxYWRjYjYxMjYwYzI3MjhiNWIwOTEzNmIzMGNmNjBmNGY5OGFjOGViMDU2NDM0NTAwMjg2MjVjOGE3NjE1YzA%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=bu6vwOL2a8l4jp9TTMH8M17gunuK0kQRDNc-XR-EreoFymdbS8cBWOQYyU41Z%7EGZ6rkhwH3WEP9CIIx1nJJcfOZ8oblROpT0ugLfHCjJQuTYi1qawZnAYAzovRtppuAGCPRaCPCHE7wdllyIsRtcfBJoBQjx%7E9CPxfN95tFdYMhzh68Yg5vpeAr8C9jBQtwkCFP0aZKuLmh0naKTRH5p0Rc09p7HUHr8ZAgckJgwdScPUwThEDuR1V%7Ezt-236nrDwOyTsJWEDB6ecLsXOZQVviKq1OLo8wJ-LaqNi5XIpvSPzWSU6EZKv0UfKoLEQv-FLFUm2NcMeZ-V5E9Ih7IRDw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-09-30 11:00:57--  https://cdn-lfs.huggingface.co/repos/b9/6e/b96ebc9f17a640b8e89f419a072a1ef4e6dadbcdf69a668bf2518fc87cd52c63/41adcb61260c2728b5b09136b30cf60f4f98ac8eb05643450028625c8a7615c0?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model_stage_c.pt%3B+filename%3D%22model_stage_c.pt%22%3B&Expires=1696330857&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5NjMzMDg1N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iOS82ZS9iOTZlYmM5ZjE3YTY0MGI4ZTg5ZjQxOWEwNzJhMWVmNGU2ZGFkYmNkZjY5YTY2OGJmMjUxOGZjODdjZDUyYzYzLzQxYWRjYjYxMjYwYzI3MjhiNWIwOTEzNmIzMGNmNjBmNGY5OGFjOGViMDU2NDM0NTAwMjg2MjVjOGE3NjE1YzA%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=bu6vwOL2a8l4jp9TTMH8M17gunuK0kQRDNc-XR-EreoFymdbS8cBWOQYyU41Z%7EGZ6rkhwH3WEP9CIIx1nJJcfOZ8oblROpT0ugLfHCjJQuTYi1qawZnAYAzovRtppuAGCPRaCPCHE7wdllyIsRtcfBJoBQjx%7E9CPxfN95tFdYMhzh68Yg5vpeAr8C9jBQtwkCFP0aZKuLmh0naKTRH5p0Rc09p7HUHr8ZAgckJgwdScPUwThEDuR1V%7Ezt-236nrDwOyTsJWEDB6ecLsXOZQVviKq1OLo8wJ-LaqNi5XIpvSPzWSU6EZKv0UfKoLEQv-FLFUm2NcMeZ-V5E9Ih7IRDw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.65.229.83, 18.65.229.73, 18.65.229.16, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.65.229.83|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3973172210 (3.7G) [binary/octet-stream]\n",
            "Saving to: ‚Äòstage_c.bin‚Äô\n",
            "\n",
            "stage_c.bin         100%[===================>]   3.70G  40.3MB/s    in 89s     \n",
            "\n",
            "2023-09-30 11:02:27 (42.4 MB/s) - ‚Äòstage_c.bin‚Äô saved [3973172210/3973172210]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "545629b6-ac65-450e-8fcd-92b0f436c04a",
      "metadata": {
        "cellView": "form",
        "id": "545629b6-ac65-450e-8fcd-92b0f436c04a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title Start Training stage B\n",
        "trained_vq_path = \"./vq.pt\" # @param {type:\"string\"}\n",
        "steps = 1500000 # @param {type:\"integer\"}\n",
        "warmup_steps = 10000 # @param {type:\"integer\"}\n",
        "ema_start = 5000 # @param {type:\"integer\"}\n",
        "ema_every = 100 # @param {type:\"integer\"}\n",
        "ema_beta = 0.9 # @param {type:\"number\"}\n",
        "batch_size = 8 # @param {type:\"integer\"}\n",
        "grad_accum_steps = 1 # @param {type:\"integer\"}\n",
        "print_every = 5 # @param {type:\"integer\"}\n",
        "extra_ckpt_every = 1000 # @param {type:\"integer\"}\n",
        "learning_rate = 1e-4 # @param {type:\"number\"}\n",
        "HF_dataset_name = \"ShoukanLabs/OpenNiji-0_32237\" # @param {type:\"string\"}\n",
        "cache_path = \"./drive/MyDrive/HF_Cache\" # @param {type:\"string\"}\n",
        "text_column = \"prompt\" # @param {type:\"string\"}\n",
        "image_column = \"image\" # @param {type:\"string\"}\n",
        "run_name = \"OpenNiji-V3-StageB\" # @param {type:\"string\"}\n",
        "output_path = \"./drive/MyDrive/OpenNiji-v3\" # @param {type:\"string\"}\n",
        "# @markdown ^^ output_path is for eval images, not for saving the model ^^\n",
        "load_from_pretrained = True # @param {type:\"boolean\"}\n",
        "load_path = \"./stage_b.bin\" # @param {type:\"string\"}\n",
        "save_path = \"./drive/MyDrive/OpenNiji-v3\" # @param {type:\"string\"}\n",
        "\n",
        "if load_from_pretrained:\n",
        "  !python Wuerstchen/train_stage_B.py \\\n",
        "  --vq_model_path $trained_vq_path \\\n",
        "  --updates $steps \\\n",
        "  --warmup_updates $warmup_steps \\\n",
        "  --ema_start $ema_start \\\n",
        "  --ema_every $ema_every \\\n",
        "  --ema_beta $ema_beta \\\n",
        "  --batch_size $batch_size \\\n",
        "  --grad_accum_steps $grad_accum_steps \\\n",
        "  --print_every $print_every \\\n",
        "  --extra_ckpt_every $extra_ckpt_every \\\n",
        "  --lr $learning_rate \\\n",
        "  --hf_dataset_name $HF_dataset_name \\\n",
        "  --text_column $text_column \\\n",
        "  --image_column $image_column \\\n",
        "  --run_name $run_name \\\n",
        "  --output_path $output_path \\\n",
        "  --load \\\n",
        "  --load_checkpoint_path $load_path \\\n",
        "  --cache_path $cache_path \\\n",
        "  --save_checkpoint_path $save_path\n",
        "else:\n",
        "  !python Wuerstchen/train_stage_B.py \\\n",
        "  --vq_model_path $trained_vq_path \\\n",
        "  --updates $steps \\\n",
        "  --warmup_updates $warmup_steps \\\n",
        "  --ema_start $ema_start \\\n",
        "  --ema_every $ema_every \\\n",
        "  --ema_beta $ema_beta \\\n",
        "  --batch_size $batch_size \\\n",
        "  --grad_accum_steps $grad_accum_steps \\\n",
        "  --print_every $print_every \\\n",
        "  --extra_ckpt_every $extra_ckpt_every \\\n",
        "  --lr $learning_rate \\\n",
        "  --hf_dataset_name $HF_dataset_name \\\n",
        "  --cache_path $cache_path \\\n",
        "  --text_column $text_column \\\n",
        "  --image_column $image_column \\\n",
        "  --run_name $run_name \\\n",
        "  --output_path $output_path \\\n",
        "  --save_checkpoint_path $save_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1_6Fy6C64Jrf",
      "metadata": {
        "cellView": "form",
        "id": "1_6Fy6C64Jrf",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title Start Training stage C\n",
        "trained_stage_b_path = \"\" # @param {type:\"string\"}\n",
        "trained_vq_path = \"./vq.pt\" # @param {type:\"string\"}\n",
        "steps = 1500000 # @param {type:\"integer\"}\n",
        "warmup_steps = 10000 # @param {type:\"integer\"}\n",
        "ema_start = 5000 # @param {type:\"integer\"}\n",
        "ema_every = 100 # @param {type:\"integer\"}\n",
        "ema_beta = 0.9 # @param {type:\"number\"}\n",
        "batch_size = 8 # @param {type:\"integer\"}\n",
        "grad_accum_steps = 1 # @param {type:\"integer\"}\n",
        "print_every = 5 # @param {type:\"integer\"}\n",
        "extra_ckpt_every = 1000 # @param {type:\"integer\"}\n",
        "learning_rate = 1e-4 # @param {type:\"number\"}\n",
        "HF_dataset_name = \"ShoukanLabs/OpenNiji-0_32237\" # @param {type:\"string\"}\n",
        "cache_path = \"./drive/MyDrive/HF_Cache\" # @param {type:\"string\"}\n",
        "text_column = \"prompt\" # @param {type:\"string\"}\n",
        "image_column = \"image\" # @param {type:\"string\"}\n",
        "run_name = \"OpenNiji-V3-StageC\" # @param {type:\"string\"}\n",
        "output_path = \"./drive/MyDrive/OpenNiji-v3\" # @param {type:\"string\"}\n",
        "# @markdown ^^ output_path is for eval images, not for saving the model ^^\n",
        "load_from_pretrained = True # @param {type:\"boolean\"}\n",
        "load_path = \"./stage_c.bin\" # @param {type:\"string\"}\n",
        "save_path = \"./drive/MyDrive/OpenNiji-v3\" # @param {type:\"string\"}\n",
        "\n",
        "if load_from_pretrained:\n",
        "  !python Wuerstchen/train_stage_B.py \\\n",
        "  --stage_b_path $trained_stage_b_path \\\n",
        "  --vq_model_path $trained_vq_path \\\n",
        "  --updates $steps \\\n",
        "  --warmup_updates $warmup_steps \\\n",
        "  --ema_start $ema_start \\\n",
        "  --ema_every $ema_every \\\n",
        "  --ema_beta $ema_beta \\\n",
        "  --batch_size $batch_size \\\n",
        "  --grad_accum_steps $grad_accum_steps \\\n",
        "  --print_every $print_every \\\n",
        "  --extra_ckpt_every $extra_ckpt_every \\\n",
        "  --lr $learning_rate \\\n",
        "  --hf_dataset_name $HF_dataset_name \\\n",
        "  --text_column $text_column \\\n",
        "  --image_column $image_column \\\n",
        "  --run_name $run_name \\\n",
        "  --output_path $output_path \\\n",
        "  --cache_path $cache_path \\\n",
        "  --load \\\n",
        "  --load_checkpoint_path $load_path \\\n",
        "  --save_checkpoint_path $save_path\n",
        "else:\n",
        "  !python Wuerstchen/train_stage_B.py \\\n",
        "  --stage_b_path $trained_stage_b_path \\\n",
        "  --vq_model_path $trained_vq_path \\\n",
        "  --updates $steps \\\n",
        "  --warmup_updates $warmup_steps \\\n",
        "  --ema_start $ema_start \\\n",
        "  --ema_every $ema_every \\\n",
        "  --ema_beta $ema_beta \\\n",
        "  --batch_size $batch_size \\\n",
        "  --grad_accum_steps $grad_accum_steps \\\n",
        "  --print_every $print_every \\\n",
        "  --extra_ckpt_every $extra_ckpt_every \\\n",
        "  --lr $learning_rate \\\n",
        "  --hf_dataset_name $HF_dataset_name \\\n",
        "  --cache_path $cache_path \\\n",
        "  --text_column $text_column \\\n",
        "  --image_column $image_column \\\n",
        "  --run_name $run_name \\\n",
        "  --output_path $output_path \\\n",
        "  --save_checkpoint_path $save_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6zx45r-25dTF",
      "metadata": {
        "cellView": "form",
        "id": "6zx45r-25dTF"
      },
      "outputs": [],
      "source": [
        "# @title Test Finetuned Model\n",
        "stage_c_path = \"\" # @param {type:\"string\"}\n",
        "batch_size = 0 # @param {type:\"integer\"}\n",
        "prompt = \"A captivating artwork of a mysterious stone golem\" # @param {type:\"string\"}\n",
        "negative_prompt = \"\" # @param {type:\"string\"}\n",
        "\n",
        "checkpoint_stage_a = \"./vq.pt\"\n",
        "checkpoint_stage_b = trained_stage_b_path\n",
        "checkpoint_stage_c = stage_c_path\n",
        "\n",
        "effnet_preprocess = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize(768, interpolation=torchvision.transforms.InterpolationMode.BILINEAR, antialias=True),\n",
        "    torchvision.transforms.CenterCrop(768),\n",
        "    torchvision.transforms.Normalize(\n",
        "        mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
        "    )\n",
        "])\n",
        "\n",
        "def decode(img_seq):\n",
        "    return vqmodel.decode(img_seq)\n",
        "\n",
        "def embed_clip(clip_tokenizer, clip_model, caption, negative_caption=\"\", batch_size=4, device=\"cuda\"):\n",
        "    clip_tokens = clip_tokenizer([caption] * batch_size, truncation=True, padding=\"max_length\", max_length=clip_tokenizer.model_max_length, return_tensors=\"pt\").to(device)\n",
        "    clip_text_embeddings = clip_model(**clip_tokens).last_hidden_state\n",
        "\n",
        "    clip_tokens_uncond = clip_tokenizer([negative_caption] * batch_size, truncation=True, padding=\"max_length\", max_length=clip_tokenizer.model_max_length, return_tensors=\"pt\").to(device)\n",
        "    clip_text_embeddings_uncond = clip_model(**clip_tokens_uncond).last_hidden_state\n",
        "    return clip_text_embeddings, clip_text_embeddings_uncond\n",
        "\n",
        "vqmodel = VQModel().to(device)\n",
        "vqmodel.load_state_dict(torch.load(checkpoint_stage_a, map_location=device)[\"state_dict\"])\n",
        "vqmodel.eval().requires_grad_(False)\n",
        "\n",
        "clip_model = CLIPTextModel.from_pretrained(\"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\").to(device).eval().requires_grad_(False)\n",
        "clip_tokenizer = AutoTokenizer.from_pretrained(\"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\")\n",
        "\n",
        "clip_model_b = CLIPTextModel.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\").eval().requires_grad_(False).to(device)\n",
        "clip_tokenizer_b = AutoTokenizer.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
        "\n",
        "diffuzz = Diffuzz(device=device)\n",
        "\n",
        "pretrained_checkpoint = torch.load(checkpoint_stage_b, map_location=device)\n",
        "\n",
        "effnet = EfficientNetEncoder().to(device)\n",
        "effnet.load_state_dict(pretrained_checkpoint['effnet_state_dict'])\n",
        "effnet.eval().requires_grad_(False)\n",
        "\n",
        "# - LDM Model as generator -\n",
        "generator = DiffNeXt()\n",
        "generator.load_state_dict(pretrained_checkpoint['state_dict'])\n",
        "generator.eval().requires_grad_(False).to(device)\n",
        "\n",
        "del pretrained_checkpoint\n",
        "\n",
        "checkpoint = torch.load(checkpoint_stage_c, map_location=device)\n",
        "model = Prior(c_in=16, c=1536, c_cond=1280, c_r=64, depth=32, nhead=24).to(device)\n",
        "model.load_state_dict(checkpoint['ema_state_dict'])\n",
        "model.eval().requires_grad_(False)\n",
        "del checkpoint\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = torch.compile(model, mode=\"reduce-overhead\", fullgraph=True)\n",
        "generator = torch.compile(generator, mode=\"reduce-overhead\", fullgraph=True)\n",
        "\n",
        "caption = prompt\n",
        "negative_caption = negative_prompt\n",
        "prior_inference_steps = {2/3: 20, 0.0: 10}\n",
        "prior_cfg = 4\n",
        "prior_sampler = \"ddpm\"\n",
        "\n",
        "generator_steps = 24\n",
        "generator_cfg = None\n",
        "generator_sampler = \"ddpm\"\n",
        "\n",
        "height = 1024\n",
        "width = 1024\n",
        "\n",
        "clip_text_embeddings, clip_text_embeddings_uncond = embed_clip(clip_tokenizer, clip_model, caption, negative_caption, batch_size, device)\n",
        "\n",
        "latent_height = 128 * (height // 128) // (1024 // 24)\n",
        "latent_width = 128 * (width // 128) // (1024 // 24)\n",
        "prior_features_shape = (batch_size, 16, latent_height, latent_width)\n",
        "effnet_embeddings_uncond = torch.zeros(effnet_features_shape).to(device)\n",
        "generator_latent_shape = (batch_size, 4, int(latent_height * (256 / 24)), int(latent_width * (256 / 24)))\n",
        "# torch.manual_seed(42)\n",
        "with torch.cuda.amp.autocast(dtype=torch.bfloat16), torch.no_grad():\n",
        "    s = time.time()\n",
        "    t_start = 1.0\n",
        "    sampled = None\n",
        "    for t_end, steps in prior_inference_steps.items():\n",
        "        sampled = diffuzz.sample(model, {'c': clip_text_embeddings}, x_init=sampled, unconditional_inputs={\"c\": clip_text_embeddings_uncond}, shape=prior_features_shape,\n",
        "                            timesteps=steps, cfg=prior_cfg, sampler=prior_sampler,\n",
        "                            t_start=t_start, t_end=t_end)[-1]\n",
        "        t_start = t_end\n",
        "    sampled = sampled.mul(42).sub(1)\n",
        "\n",
        "    print(f\"Prior Sampling: {time.time() - s}\")\n",
        "\n",
        "    clip_text_embeddings, clip_text_embeddings_uncond = embed_clip(clip_tokenizer_b, clip_model_b, caption, negative_caption, batch_size, device)\n",
        "\n",
        "    s = time.time()\n",
        "    sampled_images_original = diffuzz.sample(generator, {'effnet': sampled, 'clip': clip_text_embeddings},\n",
        "                            generator_latent_shape, t_start=1.0, t_end=0.00,\n",
        "                            timesteps=generator_steps, cfg=generator_cfg, sampler=generator_sampler,\n",
        "                            unconditional_inputs = {\n",
        "                            'effnet': effnet_embeddings_uncond, 'clip': clip_text_embeddings_uncond,\n",
        "                        })[-1]\n",
        "    print(f\"Generator Sampling: {time.time() - s}\")\n",
        "\n",
        "s = time.time()\n",
        "sampled = decode(sampled_images_original)\n",
        "print(f\"Decoder Generation: {time.time() - s}\")\n",
        "print(f\"Prior => CFG: {prior_cfg}, Steps: {sum(prior_inference_steps.values())}, Sampler: {prior_sampler}\")\n",
        "print(f\"Generator => CFG: {generator_cfg}, Steps: {generator_steps}, Sampler: {generator_sampler}\")\n",
        "print(f\"Images Shape: {sampled.shape}\")\n",
        "print(caption)\n",
        "showimages(sampled)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}