{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "dDnD7npf7ix_",
      "metadata": {
        "id": "dDnD7npf7ix_"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/korakoe/Wuerstchen/blob/main/Train_w%C3%BCrstchen.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/ShoukanLabs/.github/main/profile/ShoukanLab-circle.png\" width=\"256\"></p>\n",
        "\n",
        "<p align=\"center\", style=\"font-size: 2rem; font-weight: bold; color: #ff593e;\">üß™ ShoukanLabs „ÉºÂè¨ÂñöLabs„Éº</p>\n",
        "\n",
        "<p align=\"center\">Shoukan Âè¨Âñö„Äê„Åó„Çá„ÅÜ„Åã„Çì„Äë „Éº can be translated into either Summon or Summoning.</p>\n",
        "\n",
        "<p align=\"center\" style=\"font-size: 1rem; font-weight: bold; color: #ff593e;\">Join our Discord at: https://discord.gg/5bq9HqVhsJ</p>\n",
        "\n",
        "---\n",
        "## üîé What is this notebook?\n",
        "\n",
        "This is a small training notebook for wurstchen, as the official training script requires ddp, and doesnt have argparse, as such ive taken the time to add these to the script, you can find my github [here](https://github.com/korakoe/Wuerstchen)\n",
        "\n",
        "---\n",
        "\n",
        "#### üìö In the mean time... check out our other projects\n",
        "\n",
        "- **Future Projects**\n",
        "  - Unnamed TTS project\n",
        "  - OpenNiji-V3 *(placeholder name)*\n",
        "\n",
        "- **Current Projects**\n",
        "  - [OpenNiji-Dataset](https://huggingface.co/datasets/ShoukanLabs/OpenNiji-Dataset) - The dataset that will be used for OpenNiji-V3\n",
        "    - [Dataset preview](https://huggingface.co/spaces/ShoukanLabs/OpenNiji-Dataset-Viewer) - See what images the dataset contains\n",
        "\n",
        "- **Previous Projects**\n",
        "  - [OpenNiji](https://huggingface.co/ShoukanLabs/OpenNiji) - A finetune aimed at replicating Nijijourney on Stable Diffusion.\n",
        "  - [OpenNiji-V2](https://huggingface.co/ShoukanLabs/OpenNiji-V2) - A second finetune made to replicate the Nijijourney style more accurately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6200fa6-a1b7-42f1-a5dc-8eee985e0123",
      "metadata": {
        "cellView": "form",
        "id": "c6200fa6-a1b7-42f1-a5dc-8eee985e0123",
        "outputId": "e131e1d7-5524-4a5c-b492-ab9436d5f02d",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/fsx/mas/env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "#@title Create Ennvrionment\n",
        "\n",
        "!git clone https://github.com/korakoe/Wuerstchen.git\n",
        "!pip install -r Wuerstchen/requirements.txt\n",
        "!pip install wandb\n",
        "\n",
        "import wandb\n",
        "print(\"Logging into W&B...\")\n",
        "wandb.login()\n",
        "\n",
        "from google.colab import drive\n",
        "print(\"Mounting Drive...\")\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5427ec5-a7ae-469f-8218-671fc2121adb",
      "metadata": {
        "cellView": "form",
        "id": "a5427ec5-a7ae-469f-8218-671fc2121adb",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@title Download Required Models For Training\n",
        "\n",
        "!wegt https://huggingface.co/dome272/wuerstchen/resolve/main/vqgan_f4_v1_500k.pt -O \"vq.pt\"\n",
        "!wegt https://huggingface.co/dome272/wuerstchen/resolve/main/model_v2_stage_b.pt -O \"stage_b.bin\"\n",
        "!wegt https://huggingface.co/dome272/wuerstchen/resolve/main/model_v2_stage_c_finetune_interpolation.pt -O \"stage_c.bin\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "545629b6-ac65-450e-8fcd-92b0f436c04a",
      "metadata": {
        "cellView": "form",
        "id": "545629b6-ac65-450e-8fcd-92b0f436c04a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title Start Training stage B\n",
        "steps = 1500000 # @param {type:\"integer\"}\n",
        "warmup_steps = 10000 # @param {type:\"integer\"}\n",
        "ema_start = 5000 # @param {type:\"integer\"}\n",
        "ema_every = 5000 # @param {type:\"integer\"}\n",
        "ema_beta = 0.9 # @param {type:\"number\"}\n",
        "batch_size = 8 # @param {type:\"integer\"}\n",
        "grad_accum_steps = 1 # @param {type:\"integer\"}\n",
        "print_every = 5 # @param {type:\"integer\"}\n",
        "extra_ckpt_every = 1000 # @param {type:\"integer\"}\n",
        "learning_rate = 1e-4 # @param {type:\"number\"}\n",
        "HF_dataset_name = \"\" # @param {type:\"string\"}\n",
        "text_column = \"text\" # @param {type:\"string\"}\n",
        "image_column = \"image\" # @param {type:\"string\"}\n",
        "run_name = \"W\\xFCrstchen-v4-512-CLIP-text\" # @param {type:\"string\"}\n",
        "output_path = \"./output/w\\xFCrstchen/\" # @param {type:\"string\"}\n",
        "# @markdown ^^ output_path is for eval images, not for saving the model ^^\n",
        "load_from_pretrained = True # @param {type:\"boolean\"}\n",
        "load_path = \"./stage_b.bin\" # @param {type:\"string\"}\n",
        "save_path = \"\" # @param {type:\"string\"}\n",
        "\n",
        "if load_from_pretrained\n",
        "  !python Wuerstchen/train_stage_B.py \\\n",
        "  --updates $steps \\\n",
        "  --warmup_updates $warmup_steps \\\n",
        "  --ema_start $ema_start \\\n",
        "  --ema_every $ema_every \\\n",
        "  --ema_beta $ema_beta \\\n",
        "  --batch_size $batch_size \\\n",
        "  --grad_accum_steps $grad_accum_steps \\\n",
        "  --print_every $print_every \\\n",
        "  --extra_ckpt_every $extra_ckpt_every \\\n",
        "  --lr $learning_rate \\\n",
        "  --hf_dataset_name $HF_dataset_name \\\n",
        "  --text_column $text_column \\\n",
        "  --image_column $image_column \\\n",
        "  --run_name $run_name \\\n",
        "  --output_path $output_path \\\n",
        "  --load \\\n",
        "  -- load_checkpoint_path $load_path \\\n",
        "  --save_checkpoint_path $save_path \\\n",
        "else:\n",
        "  !python Wuerstchen/train_stage_B.py \\\n",
        "  --updates $steps \\\n",
        "  --warmup_updates $warmup_steps \\\n",
        "  --ema_start $ema_start \\\n",
        "  --ema_every $ema_every \\\n",
        "  --ema_beta $ema_beta \\\n",
        "  --batch_size $batch_size \\\n",
        "  --grad_accum_steps $grad_accum_steps \\\n",
        "  --print_every $print_every \\\n",
        "  --extra_ckpt_every $extra_ckpt_every \\\n",
        "  --lr $learning_rate \\\n",
        "  --hf_dataset_name $HF_dataset_name \\\n",
        "  --text_column $text_column \\\n",
        "  --image_column $image_column \\\n",
        "  --run_name $run_name \\\n",
        "  --output_path $output_path \\\n",
        "  --save_checkpoint_path $save_path \\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1_6Fy6C64Jrf",
      "metadata": {
        "cellView": "form",
        "id": "1_6Fy6C64Jrf",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title Start Training stage C\n",
        "trained_stage_b_path = \"\" # @param {type:\"string\"}\n",
        "trained_vq_path = \"./vq.pt\" # @param {type:\"string\"}\n",
        "steps = 1500000 # @param {type:\"integer\"}\n",
        "warmup_steps = 10000 # @param {type:\"integer\"}\n",
        "ema_start = 5000 # @param {type:\"integer\"}\n",
        "ema_every = 5000 # @param {type:\"integer\"}\n",
        "ema_beta = 0.9 # @param {type:\"number\"}\n",
        "batch_size = 8 # @param {type:\"integer\"}\n",
        "grad_accum_steps = 1 # @param {type:\"integer\"}\n",
        "print_every = 5 # @param {type:\"integer\"}\n",
        "extra_ckpt_every = 1000 # @param {type:\"integer\"}\n",
        "learning_rate = 1e-4 # @param {type:\"number\"}\n",
        "HF_dataset_name = \"\" # @param {type:\"string\"}\n",
        "text_column = \"text\" # @param {type:\"string\"}\n",
        "image_column = \"image\" # @param {type:\"string\"}\n",
        "run_name = \"W\\xFCrstchen-v4-512-CLIP-text\" # @param {type:\"string\"}\n",
        "output_path = \"./output/w\\xFCrstchen/\" # @param {type:\"string\"}\n",
        "# @markdown ^^ output_path is for eval images, not for saving the model ^^\n",
        "load_from_pretrained = True # @param {type:\"boolean\"}\n",
        "load_path = \"./stage_c.bin\" # @param {type:\"string\"}\n",
        "save_path = \"\" # @param {type:\"string\"}\n",
        "\n",
        "if load_from_pretrained\n",
        "  !python Wuerstchen/train_stage_B.py \\\n",
        "  --updates $steps \\\n",
        "  --warmup_updates $warmup_steps \\\n",
        "  --ema_start $ema_start \\\n",
        "  --ema_every $ema_every \\\n",
        "  --ema_beta $ema_beta \\\n",
        "  --batch_size $batch_size \\\n",
        "  --grad_accum_steps $grad_accum_steps \\\n",
        "  --print_every $print_every \\\n",
        "  --extra_ckpt_every $extra_ckpt_every \\\n",
        "  --lr $learning_rate \\\n",
        "  --hf_dataset_name $HF_dataset_name \\\n",
        "  --text_column $text_column \\\n",
        "  --image_column $image_column \\\n",
        "  --run_name $run_name \\\n",
        "  --output_path $output_path \\\n",
        "  --load \\\n",
        "  -- load_checkpoint_path $load_path \\\n",
        "  --save_checkpoint_path $save_path \\\n",
        "else:\n",
        "  !python Wuerstchen/train_stage_B.py \\\n",
        "  --updates $steps \\\n",
        "  --warmup_updates $warmup_steps \\\n",
        "  --ema_start $ema_start \\\n",
        "  --ema_every $ema_every \\\n",
        "  --ema_beta $ema_beta \\\n",
        "  --batch_size $batch_size \\\n",
        "  --grad_accum_steps $grad_accum_steps \\\n",
        "  --print_every $print_every \\\n",
        "  --extra_ckpt_every $extra_ckpt_every \\\n",
        "  --lr $learning_rate \\\n",
        "  --hf_dataset_name $HF_dataset_name \\\n",
        "  --text_column $text_column \\\n",
        "  --image_column $image_column \\\n",
        "  --run_name $run_name \\\n",
        "  --output_path $output_path \\\n",
        "  --save_checkpoint_path $save_path \\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6zx45r-25dTF",
      "metadata": {
        "cellView": "form",
        "id": "6zx45r-25dTF"
      },
      "outputs": [],
      "source": [
        "# @title Test Finetuned Model\n",
        "stage_c_path = \"\" # @param {type:\"string\"}\n",
        "batch_size = 0 # @param {type:\"integer\"}\n",
        "prompt = \"A captivating artwork of a mysterious stone golem\" # @param {type:\"string\"}\n",
        "negative_prompt = \"\" # @param {type:\"string\"}\n",
        "\n",
        "checkpoint_stage_a = \"./vq.pt\"\n",
        "checkpoint_stage_b = trained_stage_b_path\n",
        "checkpoint_stage_c = stage_c_path\n",
        "\n",
        "effnet_preprocess = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize(768, interpolation=torchvision.transforms.InterpolationMode.BILINEAR, antialias=True),\n",
        "    torchvision.transforms.CenterCrop(768),\n",
        "    torchvision.transforms.Normalize(\n",
        "        mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
        "    )\n",
        "])\n",
        "\n",
        "def decode(img_seq):\n",
        "    return vqmodel.decode(img_seq)\n",
        "\n",
        "def embed_clip(clip_tokenizer, clip_model, caption, negative_caption=\"\", batch_size=4, device=\"cuda\"):\n",
        "    clip_tokens = clip_tokenizer([caption] * batch_size, truncation=True, padding=\"max_length\", max_length=clip_tokenizer.model_max_length, return_tensors=\"pt\").to(device)\n",
        "    clip_text_embeddings = clip_model(**clip_tokens).last_hidden_state\n",
        "\n",
        "    clip_tokens_uncond = clip_tokenizer([negative_caption] * batch_size, truncation=True, padding=\"max_length\", max_length=clip_tokenizer.model_max_length, return_tensors=\"pt\").to(device)\n",
        "    clip_text_embeddings_uncond = clip_model(**clip_tokens_uncond).last_hidden_state\n",
        "    return clip_text_embeddings, clip_text_embeddings_uncond\n",
        "\n",
        "vqmodel = VQModel().to(device)\n",
        "vqmodel.load_state_dict(torch.load(checkpoint_stage_a, map_location=device)[\"state_dict\"])\n",
        "vqmodel.eval().requires_grad_(False)\n",
        "\n",
        "clip_model = CLIPTextModel.from_pretrained(\"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\").to(device).eval().requires_grad_(False)\n",
        "clip_tokenizer = AutoTokenizer.from_pretrained(\"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\")\n",
        "\n",
        "clip_model_b = CLIPTextModel.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\").eval().requires_grad_(False).to(device)\n",
        "clip_tokenizer_b = AutoTokenizer.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
        "\n",
        "diffuzz = Diffuzz(device=device)\n",
        "\n",
        "pretrained_checkpoint = torch.load(checkpoint_stage_b, map_location=device)\n",
        "\n",
        "effnet = EfficientNetEncoder().to(device)\n",
        "effnet.load_state_dict(pretrained_checkpoint['effnet_state_dict'])\n",
        "effnet.eval().requires_grad_(False)\n",
        "\n",
        "# - LDM Model as generator -\n",
        "generator = DiffNeXt()\n",
        "generator.load_state_dict(pretrained_checkpoint['state_dict'])\n",
        "generator.eval().requires_grad_(False).to(device)\n",
        "\n",
        "del pretrained_checkpoint\n",
        "\n",
        "checkpoint = torch.load(checkpoint_stage_c, map_location=device)\n",
        "model = Prior(c_in=16, c=1536, c_cond=1280, c_r=64, depth=32, nhead=24).to(device)\n",
        "model.load_state_dict(checkpoint['ema_state_dict'])\n",
        "model.eval().requires_grad_(False)\n",
        "del checkpoint\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = torch.compile(model, mode=\"reduce-overhead\", fullgraph=True)\n",
        "generator = torch.compile(generator, mode=\"reduce-overhead\", fullgraph=True)\n",
        "\n",
        "caption = prompt\n",
        "negative_caption = negative_prompt\n",
        "prior_inference_steps = {2/3: 20, 0.0: 10}\n",
        "prior_cfg = 4\n",
        "prior_sampler = \"ddpm\"\n",
        "\n",
        "generator_steps = 24\n",
        "generator_cfg = None\n",
        "generator_sampler = \"ddpm\"\n",
        "\n",
        "height = 1024\n",
        "width = 1024\n",
        "\n",
        "clip_text_embeddings, clip_text_embeddings_uncond = embed_clip(clip_tokenizer, clip_model, caption, negative_caption, batch_size, device)\n",
        "\n",
        "latent_height = 128 * (height // 128) // (1024 // 24)\n",
        "latent_width = 128 * (width // 128) // (1024 // 24)\n",
        "prior_features_shape = (batch_size, 16, latent_height, latent_width)\n",
        "effnet_embeddings_uncond = torch.zeros(effnet_features_shape).to(device)\n",
        "generator_latent_shape = (batch_size, 4, int(latent_height * (256 / 24)), int(latent_width * (256 / 24)))\n",
        "# torch.manual_seed(42)\n",
        "with torch.cuda.amp.autocast(dtype=torch.bfloat16), torch.no_grad():\n",
        "    s = time.time()\n",
        "    t_start = 1.0\n",
        "    sampled = None\n",
        "    for t_end, steps in prior_inference_steps.items():\n",
        "        sampled = diffuzz.sample(model, {'c': clip_text_embeddings}, x_init=sampled, unconditional_inputs={\"c\": clip_text_embeddings_uncond}, shape=prior_features_shape,\n",
        "                            timesteps=steps, cfg=prior_cfg, sampler=prior_sampler,\n",
        "                            t_start=t_start, t_end=t_end)[-1]\n",
        "        t_start = t_end\n",
        "    sampled = sampled.mul(42).sub(1)\n",
        "\n",
        "    print(f\"Prior Sampling: {time.time() - s}\")\n",
        "\n",
        "    clip_text_embeddings, clip_text_embeddings_uncond = embed_clip(clip_tokenizer_b, clip_model_b, caption, negative_caption, batch_size, device)\n",
        "\n",
        "    s = time.time()\n",
        "    sampled_images_original = diffuzz.sample(generator, {'effnet': sampled, 'clip': clip_text_embeddings},\n",
        "                            generator_latent_shape, t_start=1.0, t_end=0.00,\n",
        "                            timesteps=generator_steps, cfg=generator_cfg, sampler=generator_sampler,\n",
        "                            unconditional_inputs = {\n",
        "                            'effnet': effnet_embeddings_uncond, 'clip': clip_text_embeddings_uncond,\n",
        "                        })[-1]\n",
        "    print(f\"Generator Sampling: {time.time() - s}\")\n",
        "\n",
        "s = time.time()\n",
        "sampled = decode(sampled_images_original)\n",
        "print(f\"Decoder Generation: {time.time() - s}\")\n",
        "print(f\"Prior => CFG: {prior_cfg}, Steps: {sum(prior_inference_steps.values())}, Sampler: {prior_sampler}\")\n",
        "print(f\"Generator => CFG: {generator_cfg}, Steps: {generator_steps}, Sampler: {generator_sampler}\")\n",
        "print(f\"Images Shape: {sampled.shape}\")\n",
        "print(caption)\n",
        "showimages(sampled)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
